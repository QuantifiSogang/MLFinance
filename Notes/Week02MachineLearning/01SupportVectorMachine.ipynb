{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Support Vector Machine",
   "id": "1fd5f832545a6a5c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Support Vector Regression\n",
    "\n",
    "1995년, Vapnik와 Cortes는 Support Vector Network라는 모형을 제안하였다\n",
    "\n",
    "```\n",
    "sklearn.svm.SVR(*, kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n",
    "```\n",
    "\n",
    "주요 Hyper Parameter는 다음과 같다\n",
    "\n",
    "- kernel : SVM 커널 함수를 지정한다\n",
    "- degree : kernel을 poly, 즉 비선형으로 지정하였을 때 지정하는 차수이다. 기본값은 3이며 값이 커질수록 모형이 복잡해진다\n",
    "- coef0 : y intercept를 지정한다\n",
    "- C : L2 score (Ridge)를 비용함수로 지정한다. 값이 작아질수록 일반화 성능이 커진다. 기본값은 1이다\n",
    "\n",
    "\n",
    "https://link.springer.com/article/10.1007/BF00994018\n",
    "\n",
    "#### 1.1 Hard Margin Model\n",
    "\n",
    "Hard Margin Model의 손실함수는 다음을 최적화하는 것을 목적으로 한다\n",
    "\n",
    "$$\\mathrm{Loss_{SVR}} = \\min_w ||w||^2 + \\lambda(\\frac{1}{2}\\sum_{i = 1}^n (y_i - f(x_i))^2)$$\n",
    "\n",
    "여기서 $||w||^2$는 데이터와 회귀선과의 거리를 나타내는데, 이는 선형회귀식에서 오차를 최소화하는 식과 유사함을 알 수 있다."
   ],
   "id": "be109e77bca9fbce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d17979524e31b6cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1.2 Soft Margin Model\n",
    "\n",
    "<center><img src = \"https://leejiyoon52.github.io/images/image_67.png\" alt=\"My Image\"></center>\n",
    "\n",
    "과적합이나 기타 일반화 성능 향상을 위해, 보통은 아래와 같은 손실함수를 최적화하는 것을 목적으로 한다\n",
    "\n",
    "$$\\mathrm{Loss_{SVR}} = \\min_w \\frac{1}{2}||w||^2 + C\\sum_{i = 1}^n (\\xi_i + \\xi_i^*)$$\n",
    "$$\\mathrm{such~that~} (w^T x_i + b) - y_i \\leq \\epsilon + \\xi_i$$\n",
    "$$y_i - (w^T x_i + b) \\leq \\epsilon + \\xi_i^*$$\n",
    "$$\\xi_i, \\xi_i^* \\geq 0$$\n",
    "\n",
    "추정된 회귀식에서 상단과 하단에 각각 $\\epsilon$만큼 마진을 생성하여 추정식의 허용 범위를 넓혀 준다. 즉, margin 값을 크게 줄 수록 모형의 노이즈 허용 범위가 커지는 것이다\n",
    "\n",
    "비용함수 최적화에 있어서 Lagrangian Problem으로 해를 찾는 것이 가능하지만, 그 과정이 길기 때문에 우선은 생략한다. Lagrangian Dual Problem은 다음과 같이 정리된다\n",
    "\n",
    "$$\\mathcal{L_D} = \\frac{1}{2}\\sum_{i,j = 1}^n (\\alpha_i^* - \\alpha_i)(\\alpha_j^* - \\alpha_j) \\mathbf{x_i}^T\\mathbf{x_j} - \\epsilon \\sum_{i,j = 1}^n(\\alpha_i^* - \\alpha_i) + \\sum_{i,j = 1}^n y_i(\\alpha_i^* - \\alpha_i)$$\n",
    "$$\\mathrm{such ~ that ~} \\sum_{i=1}^n (\\alpha_i^* - \\alpha_i) = 0, ~~~~~ \\alpha_i, \\alpha_i^* \\in [0,C]$$\n",
    "\n",
    "Lagrangian dual problem으로 재구성한 목적식은 $α$로 이루어져있는 convex하고, 연속적인 quadratic programming problem이다. 최적화 프로그램을 이용한다면 간편하게 $α$를 도출할 수 있다"
   ],
   "id": "c318591e4da4b90c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bf483dfc6cd3b169"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1.3 Kernel Function\n",
    "\n",
    "Support Vector Machine은 선형 모형이기 때문에 비선형 회귀식을 구해야 하는 경우가 상당히 제약적이었다. 이런 경우 Mapping function의 일종인 Kernel Function을 사용하여 해결하게 되었다\n",
    "\n",
    "$$x = (x_1, x_2, \\dots, x_p) \\Rightarrow \\phi(x) = z = (z_1, z_2, \\dots, z_q)$$\n",
    "\n",
    "<center><img src = \"https://leejiyoon52.github.io/images/image_80.png\" alt=\"My Image\"></center>\n",
    "\n",
    "즉, 중간에 비선형 공간을 추가하여 선형공간으로 다시 데이터를 평평하게 펴는 과정을 거치는 것이라 이해하면 좋다. Kernel Function이 포함된 Lagrangian Dual Problem은 다음과 같이 표현된다\n",
    "\n",
    "$$\\mathcal{L_D} = \\frac{1}{2}\\sum_{i,j = 1}^n (\\alpha_i^* - \\alpha_i)(\\alpha_j^* - \\alpha_j)\\mathbf{K(x_i^,x_j)} - \\epsilon \\sum_{i,j = 1}^n(\\alpha_i^* - \\alpha_i) + \\sum_{i,j = 1}^n y_i(\\alpha_i^* - \\alpha_i)$$\n",
    "\n",
    "평면식과는 다르게 중간 과정에서 Kernel Function의 Mapping과정이 포함된것을 알 수 있다\n",
    "\n",
    "대표적인 kernel function에는 다음과 같은 것들이 있다. 주로 rbf가 자주 사용된다\n",
    "\n",
    "- linear : 선형 커널\n",
    "- poly : 비선형 커널, 3차 함수가 기본적으로 추정된다\n",
    "- rbf(default) : RBF커널, 가우시안 커널이다. 성능이 좋기 때문에 자주 사용된다\n",
    "- sigmoid : sigmoid 커널\n",
    "- precomputed"
   ],
   "id": "534b0576a42f0dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "aba3f40ab58c0d05"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Support Vector Classifier\n",
    "\n",
    "<center><img src = \"https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/600px-SVM_margin.png\" alt=\"My Image\"></center>\n",
    "\n",
    "Support Vector Regression의 원형은 Support Vector Machine으로, 원래 분류를 목적으로 제안된 모형이다. 1992년 Vladimir N. Vapnik와 Alexey Ya. Chervonenkis에 의해 Hard Margin model이 처음 제안되었고, 1995년 soft margin model은 Corinna Cortes and Vapnik에 의해 1995년에 제안된 모형이다. sklearn에서는 아래 함수를 통해 불러올 수 있다\n",
    "\n",
    "```\n",
    "sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, \n",
    "                probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, \n",
    "                max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\n",
    "```\n",
    "\n",
    "Regression의 Support Vector가 맨 바깥쪽에 위치한 것에 비해, Classification Model에서는 추정되는 결정 경계에 가장 가까운 vector가 support vector가 된다"
   ],
   "id": "187188c7be2fc033"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5bf62b4c97ebb23a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.1 Hard Margin Model\n",
    "\n",
    "데이터가 아래와 같이 총 $n$개 있다고 가정하자\n",
    "\n",
    "$$(x_1, y_1), \\dots, (x_n, y_n)$$\n",
    "\n",
    "여기서 $y_i$는 1과 -1으로 labeling된 정성 변수이다. SVM의 목적은 $y = -1$과 $y=1$로 labeling된 **데이터들의 거리를 최대화** 하는 것이다. 거리를 최대화하는 가상의 공간을 **Hyperplane(초평면)** 이라고 하며, 다음의 식을 만족한다\n",
    "\n",
    "$$\\mathbf{w}^T \\mathbf{x} - b = 0$$\n",
    "\n",
    "여기서 $\\mathbf {w} $는 초평면의 normal vector, $b$는 bias(=intercept)이다. 추정해야하는 parameter  ${\\frac {b}{\\|\\mathbf {w} \\|}} $는 $\\mathbf {w} $에 의해 결정된다\n",
    "\n",
    "$y = -1$과 $y=1$로 labeling된 데이터들은 다음을 만족할 것이다\n",
    "\n",
    "$${\\displaystyle \\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} _{i}-b\\geq 1\\,,{\\text{ if }}y_{i}=1}$$\n",
    "$${\\displaystyle \\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} _{i}-b\\leq -1\\,,{\\text{ if }}y_{i}=-1}$$\n",
    "\n",
    "이러한 제약 조건은 각 데이터 점이 여백의 올바른 쪽에 놓여 있어야 함을 나타내며, 다음과 같이 다시 쓸 수 있다\n",
    "\n",
    "$${\\displaystyle y_{i}(\\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} _{i}-b)\\geq 1,\\quad {\\text{ for all }}1\\leq i\\leq n}$$\n",
    "\n",
    "이를 조합하면 다음과 같은 최적화 문제를 얻을 수 있다\n",
    "\n",
    "$${\\displaystyle {\\begin{aligned}&{\\underset {\\mathbf {w} ,\\;b}{\\operatorname {minimize} }}&&\\|\\mathbf {w} \\|_{2}^{2}\\\\&{\\text{subject to}}&&y_{i}(\\mathbf {w} ^{\\top }\\mathbf {x} _{i}-b)\\geq 1\\quad \\forall i\\in \\{1,\\dots ,n\\}\\end{aligned}}}$$\n",
    "\n",
    "최적화 문제에서 추정된 $\\mathbf {w} $와 $b$는 어느 support vector들간의 거리를 최대화하는(${\\displaystyle \\mathbf {x} \\mapsto \\operatorname {sgn}(\\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} -b)}$) 값이다"
   ],
   "id": "fcbb28b9bda2daa4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "925422e9b94c1555"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.2 Soft Margin Model\n",
    "\n",
    "Hard Margin Model의 고질적인 Overfit 문제로 인해 탄생한 모형이다. Support vector간의 거리 최대화에 어느정도의 유연성을 인정한다\n",
    "\n",
    "Soft Margin Model에서 목적은 다음과 같이 달라진다\n",
    "\n",
    "$${\\displaystyle \\lambda \\lVert \\mathbf {w} \\rVert ^{2}+\\left[{\\frac {1}{n}}\\sum _{i=1}^{n}\\max \\left(0,1-y_{i}(\\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} _{i}-b)\\right)\\right]}$$\n",
    "\n",
    "여기서 parameter $\\lambda >0 $는 margin size를 조정해주는 hyper parameter이다. $\\lambda$의 값이 0에 가까울수록 모형이 더욱 엄격해지며, 큰 값이 될 수록 모형이 유연해진다. 최적화 문제는 여기서 다음과 같이 다시 정의된다\n",
    "\n",
    "$${\\displaystyle {\\begin{aligned}&{\\underset {\\mathbf {w} ,\\;b,\\;\\mathbf {\\zeta } }{\\operatorname {minimize} }}&&\\|\\mathbf {w} \\|_{2}^{2}+C\\sum _{i=1}^{n}\\zeta _{i}\\\\&{\\text{subject to}}&&y_{i}(\\mathbf {w} ^{\\top }\\mathbf {x} _{i}-b)\\geq 1-\\zeta _{i},\\quad \\zeta _{i}\\geq 0\\quad \\forall i\\in \\{1,\\dots ,n\\}\\end{aligned}}}$$\n",
    "\n",
    "$C$값이 큰 경우 입력 데이터가 선형 분류 가능한 경우 Hard Margin SVM과 유사하게 동작하지만, $C$값이 작을수록 모형이 단순해진다. 일반적으로 $C$와 $\\lambda$는 반비례 관계에 있다"
   ],
   "id": "4748d465e39b61dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c25e380901131475"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.3 Nonlinear Kernels\n",
    "\n",
    "<center><img src = \"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/1920px-Kernel_Machine.svg.png\" alt=\"My Image\"></center>\n",
    "\n",
    "Kernel function을 SVM에 적용하면 Feature map이 비선형적 공간에 선형적인 공간으로 mapping되며 비선형적 모형도 추정이 가능하다. 복잡한 모형 또한 직관적으로 해석할 수 있다는 장점이 있지만, 연산량이 비대하여 학습 시간이 굉장히 오래 걸린다. 대표적인 kernel로는 다음과 같은것이 존재한다\n",
    "\n",
    "**Polynoimal**\n",
    "\n",
    "$${\\displaystyle k(\\mathbf {x} _{i},\\mathbf {x} _{j})=(\\mathbf {x} _{i}\\cdot \\mathbf {x} _{j})^{d}}$$\n",
    "\n",
    "여기서 $d = 1$이면 lienar kernel과 같다\n",
    "\n",
    "**Gaussian radial basis function**\n",
    "\n",
    "$${\\displaystyle k(\\mathbf {x} _{i},\\mathbf {x} _{j})=\\exp \\left(-\\gamma \\left\\|\\mathbf {x} _{i}-\\mathbf {x} _{j}\\right\\|^{2}\\right)} \\mbox{ for } \\gamma >0.$$\n",
    "\n",
    "**Sigmoid function**\n",
    "\n",
    "$${\\displaystyle k(\\mathbf {x_{i}} ,\\mathbf {x_{j}} )=\\tanh(\\kappa \\mathbf {x} _{i}\\cdot \\mathbf {x} _{j}+c)} $$\n",
    "\n",
    "sklearn의 SVM에서는 kernel에서 지정 가능하다. 위에서부터 각각 `'polynomial', 'rbf', 'sigmoid'`로 지정할 수 있다. 기본값은 `rbf`이다\n",
    "\n",
    "```\n",
    "sklearn.svm.SVC(kernel = 'polynomial', degree = 3)\n",
    "sklearn.svm.SVC(kernel = 'rbf') # default\n",
    "sklearn.svm.SVC(kernel = 'sigmoid')\n",
    "```"
   ],
   "id": "537ebf534c32eaab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2c8e617489825f4c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
